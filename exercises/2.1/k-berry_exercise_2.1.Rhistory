getwd()
find_duplicates <- function(my.df) {
# Define the experimental delta, seconds
kbd <- (15 - 2)*60
# Get length of time series
n <- length(my.df$result_time)
# Define columns for checking duplicates (i.e., everything but time)
check.cols <- c("nodeid", "parent", "voltage", "humid", "humtemp", "adc0",
"adc1",   "adc2",   "adc3",    "adc4",  "adc5",    "adc6")
for (i in 1:(n-1)) {
for (j in (i+1):(n-1)) {
# Check the time delta
dt <- difftime(my.df$date[j], my.df$date[i], units = "secs")
if (dt < kbd) {
# We are in the search region; check for matching content;
# the 'all' function is TRUE if all columns match
d.mat <- all(my.df[i, check.cols] == my.df[j, check.cols])
if (d.mat) {
# In the search region w/ matching data; mark at duplicate
my.df[j, "dup"] <- 1
}
} else {
# We are passed the search region
break
}
}
}
return(my.df)
}
adc2 <- mean(tmp.df[tmp.df$adc2 >= 250 & tmp.df$adc2 <= 1000, "adc2"])
rbind?
# Initial the current time with the first time
curr.t <- t1
j]
rbind?
# Initial the current time with the first time
curr.t <- t1
rbind?
h
s2003.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2003.txt"
s2003 <- read.csv(s2003.url)
s2003
len(s2003)
length(s2003)
#Locate urls
s2003.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2003.txt"
s2015.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2015.txt"
s2025.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2025.txt"
s2045.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2045.txt"
s2055.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2055.txt"
s2065.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2065.txt"
s2085.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2085.txt"
s2095.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2095.txt"
s2103.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2103.txt"
s2115.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2115.txt"
s2125.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2125.txt"
s2135.url <- "https://raw.githubusercontent.com/ds-wm/ds-wm.github.io/master/course/atsa/data/MDA300BKUP_resultsConverted-2YR_2135.txt"
s2015 <- read.csv(s2015.url)
s2025 <- read.csv(s2025.url)
s2045 <- read.csv(s2045.url)
s2055 <- read.csv(s2055.url)
s2065 <- read.csv(s2065.url)
s2085 <- read.csv(s2085.url)
s2095 <- read.csv(s2095.url)
s2103 <- read.csv(s2103.url)
s2115 <- read.csv(s2115.url)
s2125 <- read.csv(s2125.url)
s2135 <- read.csv(s2135.url)
s2003 <- s2003[s2003$voltage >= 2600, ]
s2015 <- s2015[s2015$voltage >= 2600, ]
s2025 <- s2025[s2025$voltage >= 2600, ]
s2045 <- s2045[s2045$voltage >= 2600, ]
s2055 <- s2055[s2055$voltage >= 2600, ]
s2065 <- s2065[s2065$voltage >= 2600, ]
s2085 <- s2085[s2085$voltage >= 2600, ]
s2095 <- s2095[s2095$voltage >= 2600, ]
s2103 <- s2103[s2103$voltage >= 2600, ]
s2115 <- s2115[s2115$voltage >= 2600, ]
s2125 <- s2125[s2125$voltage >= 2600, ]
s2135 <- s2135[s2135$voltage >= 2600, ]
s2003$dup <- 0
s2015$dup <- 0
s2025$dup <- 0
s2045$dup <- 0
s2055$dup <- 0
s2065$dup <- 0
s2085$dup <- 0
s2095$dup <- 0
s2103$dup <- 0
s2115$dup <- 0
s2125$dup <- 0
s2135$dup <- 0
s2003$date <- as.POSIXct(s2003$result_time)
s2015$date <- as.POSIXct(s2015$result_time)
s2025$date <- as.POSIXct(s2025$result_time)
s2045$date <- as.POSIXct(s2045$result_time)
s2055$date <- as.POSIXct(s2055$result_time)
s2065$date <- as.POSIXct(s2065$result_time)
s2085$date <- as.POSIXct(s2085$result_time)
s2095$date <- as.POSIXct(s2095$result_time)
s2103$date <- as.POSIXct(s2103$result_time)
s2115$date <- as.POSIXct(s2115$result_time)
s2125$date <- as.POSIXct(s2125$result_time)
s2135$date <- as.POSIXct(s2135$result_time)
source('kberryb2_utility.R')
#R Utility for Exercise 2.1
#Kelton Berry
# Create function for reproducibility; this method needs run on each dataset
# ************************************************************************
# Name:     find_duplicates
# Inputs:   R data.frame with columns
#           * "nodeid", "parent", "voltage", "humid", "humtemp", "adc0"
#           * "adc1",   "adc2",   "adc3",    "adc4",  "adc5",    "adc6"
#           * "date" <- must be POSIXct object (others can be anything)
#           * "dup"  <- must be integer (0: original; 1: duplicate)
# Returns:  R data.frame (edited)
# Features: Assigns 1 to rows that are found to be duplicates based on
#           Algorithm 1 above.
# Ref:      https://doi.org/10.3390/jsan3040297
# ************************************************************************
find_duplicates <- function(my.df) {
# Define the experimental delta, seconds
kbd <- (15 - 2)*60
# Get length of time series
n <- length(my.df$result_time)
# Define columns for checking duplicates (i.e., everything but time)
check.cols <- c("nodeid", "parent", "voltage", "humid", "humtemp", "adc0",
"adc1",   "adc2",   "adc3",    "adc4",  "adc5",    "adc6")
for (i in 1:(n-1)) {
for (j in (i+1):(n-1)) {
# Check the time delta
dt <- difftime(my.df$date[j], my.df$date[i], units = "secs")
if (dt < kbd) {
# We are in the search region; check for matching content;
# the 'all' function is TRUE if all columns match
d.mat <- all(my.df[i, check.cols] == my.df[j, check.cols])
if (d.mat) {
# In the search region w/ matching data; mark at duplicate
my.df[j, "dup"] <- 1
}
} else {
# We are passed the search region
break
}
}
}
return(my.df)
}
# Create a helper function for merging a list of dataframes
# ************************************************************************
# Name:     merge_all
# Inputs:   list, list of data frames to aggregate by rows
# Returns:  R data.frame
# Features: Merges and sorts a list of data frames; rather than merge
#           just two data frames at a time.
# Ref:      https://www.r-bloggers.com/2011/01/merging-multiple-data-frames-in-r/
# ************************************************************************
merge_list <- function(listofdf){
Reduce(
function(x, y) merge(x, y, all=TRUE), listofdf)
}
#Create a helper function for aggregating by time step
agg_time <- function(t.step, mydf) {
#Define the time step for aggregation
t.step <- t.step*60
# The first instance of time and roll back to the earliest whole hour
t1 <- trunc(mydf$date[1], "hour")
# Grab the last time
tn <- tail(mydf, 1)$date
# Grab total number of rows
n <- length(mydf$result_time)
# Initialize a new empty dataframe for storing ensemble means
ensemble.df <- mydf[1==2, ]
ensemble.df <- subset(ensemble.df, select = -result_time)  # remove this column
# Initial the current time with the first time
curr.t <- t1
while (curr.t < tn) {
next.t <- curr.t + t.step
tmp.df <- orig.df[orig.df$date >= curr.t & orig.df$date < next.t, ]
#find ADCs
adc0 <- mean(tmp.df[tmp.df$adc0 >= 500 & tmp.df$adc0 <= 1000, "adc0"])
adc1 <- mean(tmp.df[tmp.df$adc1 >= 250 & tmp.df$adc1 <= 1000, "adc1"])
adc2 <- mean(tmp.df[tmp.df$adc2 >= 250 & tmp.df$adc2 <= 1000, "adc2"])
hold <- data.frame(adc0,adc1,adc2,curr.t)
names(hold) <- c("ADC0", "ADC1", "ADC2", "Curr_Time")
ensemble.df <- rbind(ensemble.df, hold)
curr.t <- curr.t + t.step
}
return(ensemble.df)
}
source('kberryb2_utility.R')
s2003 <- find_duplicates(s2003)
s2003 <- find_duplicates(s2003)
s2015 <- find_duplicates(s2015)
s2025 <- find_duplicates(s2025)
s2045 <- find_duplicates(s2045)
s2055 <- find_duplicates(s2055)
s2065 <- find_duplicates(s2065)
s2085 <- find_duplicates(s2085)
s2095 <- find_duplicates(s2095)
s2103 <- find_duplicates(s2103)
s2115 <- find_duplicates(s2115)
s2125 <- find_duplicates(s2125)
s2135 <- find_duplicates(s2135)
s2003.df <- s2003[s2003$dup == 0, ]
s2015.df <- s2015[s2015$dup == 0, ]
s2025.df <- s2025[s2025$dup == 0, ]
s2045.df <- s2045[s2045$dup == 0, ]
s2055.df <- s2055[s2055$dup == 0, ]
s2065.df <- s2065[s2065$dup == 0, ]
s2085.df <- s2085[s2085$dup == 0, ]
s2095.df <- s2095[s2095$dup == 0, ]
s2103.df <- s2103[s2103$dup == 0, ]
s2115.df <- s2115[s2115$dup == 0, ]
s2125.df <- s2125[s2125$dup == 0, ]
s2135.df <- s2135[s2135$dup == 0, ]
orig.df <- merge_list(list(s2003, s2015, s2025, s2045, s2055, s2065, s2085, s2095, s2103, s2115, s2125, s2135))
nodup.df <- merge_list(list(s2003.df, s2015.df, s2025.df, s2045.df, s2055.df, s2065.df, s2085.df, s2095.df, s2103.df, s2115.df, s2125.df, s2135.df))
orig.df <- orig.df[order(orig.df$date), ]
nodup.df <- nodup.df[order(nodup.df$date), ]
agg_fiveteen <- agg_time(15, orig.df)
agg_fiveteen <- agg_time(t.step=15, orig.df)
agg_hour <- agg_time(t.step = 60, orig.df)
agg_half <- agg_time(t.step = 12*60, orig.df)
agg_day <- agg_time(t.step = 24*60, orig.df)
#Second using the df with no duplicates
agg_fiveteen_nd <- agg_time(t.step = 15, nodup.df)
agg_hour_nd <- agg_time(t.step = 60, nodup.df)
agg_half_nd <- agg_time(t.step = 12*60, nodup.df)
agg_day_nd <- agg_time(t.step = 24*60, nodup.df)
}
}
}
calcVwc<- function(data){
# '''
# Input: dataframe column of adc1 or adc2 values
# Output: vector of converted soil moisture values
# '''
a = 0.00119
b = 0.401
output <- list()
for (i in 1:length(data)){
if (data[i] < 337 | data[i] > 841){
output[[i]] <- 0
}
else
{
output[[i]] <- a * data[i] - b
}
}
return(unlist(output))
}
# Calculate soil water potential from raw (mv) to kilo Pascals (kPa)
calcSwp <- function(data){
# '''
# Input: dataframe column of adc0 values
# Output: vector of converted soil water potential values
# '''
a <- 0.000048
b <- 0.0846
c <- 39.45
output <- list()
for (i in 1:length(data)){
if (data[i] < 591 | data[i] > 841){
output[[i]] = 0
}
else
{
output[[i]] = -1.0*exp( (a * data[i] * data[i]) - (b * data[i]) + c )
}
}
return(unlist(output))
}
install.packages("Metrics")
library("Metrics")
agg_fiveteen$adc0 <- calcSwp(agg_fiveteen$adc0)
agg_fiveteen$adc1 <- calcVwc(agg_fiveteen$adc1)
agg_fiveteen$adc2  <- calcVwc(agg_fiveteen$adc2)
agg_hour$adc0 <- calcSwp(agg_hour$adc0)
agg_hour$adc1 <- calcVwc(agg_hour$adc1)
agg_hour$adc2 <- calcVwc(agg_hour$adc2)
agg_half$adc0 <- calcSwp(agg_half$adc0)
agg_half$adc1 <- calcVwc(agg_half$adc1)
agg_half$adc2 <- calcVwc(agg_half$adc2)
agg_day$adc0  <- calcSwp(agg_day$adc0)
agg_day$adc1  <- calcVwc(agg_day$adc1)
agg_dayf$adc2  <- calcVwc(agg_day$adc2)
agg_fiveteen_nd$adc0 <- calcSwp(agg_fiveteen_nd$adc0)
agg_fiveteen_nd$adc1 <- calcVwc(agg_fiveteen_nd$adc1)
agg_fiveteen_nd$abc2  <- calcVwc(agg_fiveteen_nd$adc0)
agg_fiveteen_nd$adc2  <- calcVwc(agg_fiveteen_nd$adc2)
agg_hour_nd$adc0 <- calcSwp(agg_hour_nd$adc0)
agg_hour_nd$adc1 <- calcVwc(agg_hour_nd$adc1)
agg_hour_nd$adc2 <- calcVwc(agg_hour_nd$adc2)
agg_half_nd$adc0 <- calcSwp(agg_half_nd$adc0)
agg_half_nd$adc1 <- calcVwc(agg_half_nd$adc1)
agg_half_nd$adc2 <- calcVwc(agg_half_nd$adc2)
agg_day_nd$adc0 <- calcSwp(agg_day_nd$adc0)
agg_day_nd$adc1 <- calcVwc(agg_day_nd$adc1)
agg_day_nd$adc2 <- calcVwc(agg_day_nd$adc2)
fifteen_cor <- cor(agg_fifteen[1:3], agg_fifteen_nd[1:3])
fifteen_cor <- cor(agg_fifteen[1:3], agg_fifteen_nd[1:3], method='pearson')
fifteen_cor <- cor(agg_fifteen[1:3], agg_fifteen_nd[1:3], method='kendall')
fifteen_cor <- cor(agg_fifteen[1:3], agg_fifteen_nd[1:3], method='pearson')
hour_cor <- cor(agg_hour[1:3], agg_day_nd[1:3], method = 'pearson')
half_cor <- cor(agg_half[1:3], agg_half_nd[1:3], method = 'pearson')
day_cor <- cor(agg_day[1:3], agg_day_nd[1:3], method = 'pearson')
(fifteen_cor)
fifteen_cor
fifteen_squared <- (fifteen_cor)^2
fifteen_squared <- (fifteen_cor)^2
hour_squared <- (hour_cor)^2
half_squared <- (half_cor)^2
day_squared <- (day_cor)^2
fifteen_rmse.adc0 <- rmse(agg_fifteen$adc0, agg_fifteen_nd$adc0)
fifteen_rmse.adc1 <- rmse(agg_fifteen$adc1, agg_fifteen_nd$adc1)
fifteen_rmse.adc2 <- rmse(agg_fifteen$adc2, agg_fifteen_nd$adc2)
hour_rmse.adc0 <- rmse(agg_hour$adc0, agg_hour_nd$adc0)
hour_rmse.adc1 <- rmse(agg_hour$adc1, agg_hour_nd$adc1)
hour_rmse.adc2 <- rmse(agg_hour$adc2, agg_hour_nd$adc2)
half_rsme.adc0 <- rmse(agg_half$adc0, agg_half_nd$adc0)
half_rsme.adc1 <- rmse(agg_half$adc1, agg_half_nd$adc1)
rhalf_rsme.adc2 <- rmse(agg_half$adc2, agg_half_nd$adc2)
day_rsme.adc0 <- rmse(agg_day$adc0, agg_day_nd$adc0)
day_rsme.adc1 <- rmse(agg_day$adc1, agg_day_nd$adc1)
day_rsme.adc2 <- rmse(agg_day$adc2, agg_day_nd$adc2)
plot(agg_fifteen$adc0, agg_fifteen_np$adc0,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "15-Minute Aggregation: ADC0")
plot(agg_hour$adc0, agg_hour_np$adc0,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "Hour Aggregation: ADC0")
plot(agg_half$adc0, agg_half_np$adc0,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "12 Hour Aggregation: ADC0")
plot(agg_day$adc0, agg_day_np$adc0,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "24 Hour Aggregation: ADC0")
plot(agg_fifteen$adc2, agg_fifteen_np$adc2,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "15-Minute Aggregation: ADC2")
plot(agg_hour$adc2, agg_hour_np$adc2,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "Hour Aggregation: ADC2")
plot(agg_half$adc2, agg_half_np$adc2,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "12 Hour Aggregation: ADC2")
plot(agg_day$adc2, agg_day_np$adc2,type = 'p',
xlab = "Soil water potent (originial state)",
ylab = "Soil water potent (w/ no duplicates)",
main = "24 Hour Aggregation: ADC2")
getwd()
savehistory("~/k-berry_exercise_2.1.Rhistory")
